# -*- coding: utf-8 -*-
"""movie-recommendation-systems-honor-thesis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J3ig43fTjY_RUDKo2sf1pASpaNAruaJq

# **Movie Recommender Systems - 2022 Honor Thesis**

*Yuanrui(Jerry) Zhu*

---

> *  **Score-based Recommendation (Baseline)** - provide generalized recommendations to every user, 
based on movie overall popularity and/or genre. Since each user is different, this approach is considered to be too simple 
(could be a homepage recommendation for a movie watching website if it has no information about the user, not only the watch 
history but also the ratings of movies). The basic idea behind this system is that movies that are more popular and critically 
acclaimed will have a higher probability of being liked by the average audience. It makes sense for developing such system as
 a baseline recommendation.

> *  **Content Based Filtering** - suggest similar items based on a particular movie instance. 
This technique uses the movie metadata, such as genre, director, description, actors, etc. for movie recommendation. 
The general idea behind these recommender systems is that if a person liked a particular item, he or she will also like an item that is similar to it, measured by Cosine-Similarity. There are two recommender systems, **Plot Description Based** and **Credits, Genres and Keywords Based**, they each have their strength and weakness on some perspectives.

> *  **Collaborative Filtering (SVD)** - matches persons with similar interests and provides recommendations based on this matching among them and rating history. Collaborative filters does not use metadata like its content-based counterparts, but is able to provide a more personal suggestions.
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd 
import numpy as np 
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
from ast import literal_eval
from sklearn.feature_extraction.text import CountVectorizer
from surprise import Reader, Dataset, SVD
from surprise.model_selection import cross_validate
from sklearn.model_selection import train_test_split
from surprise.model_selection import KFold



df1 = pd.read_csv('/content/drive/MyDrive/archive 2/tmdb_5000_credits.csv')
df2 = pd.read_csv('/content/drive/MyDrive/archive 2/tmdb_5000_movies.csv')

df1.head()

df2.head()

# Rename the first dataset and merge on the second dataset using the "id" column

df1.rename(columns = {"movie_id": "id"}, inplace = True)
df2 = df2.merge(df1, on ='id')

# See the Data Type and missing values of the columns

df2.info()

"""# **Score-based Recommendation** -
   
"""

mean_rating = df2['vote_average'].mean()
percentile = df2['vote_count'].quantile(0.9)
weighted = df2.copy().loc[df2['vote_count'] >= mean_rating]

def weighted_rating(x, m=percentile, C=mean_rating):
    return (x['vote_count']/(x['vote_count']+m) * x['vote_average']) + (m/(m+x['vote_count']) * C)

weighted['score'] = weighted.apply(weighted_rating, axis=1)

#Sort movies based on score calculated above
weighted = weighted.sort_values('vote_average', ascending=False)

#Print the top 10 movies
weighted[['original_title', 'vote_count', 'vote_average']].head(10)



weighted['year'] = pd.DatetimeIndex(weighted['release_date']).year
fig, ax = plt.subplots(figsize=(15,9))

weighted.boxplot(column = 'vote_count', by = 'year', ax = ax, rot = 45, grid = False)

pop = df2.sort_values('popularity', ascending=False)

plt.figure(figsize=(12,4))
plt.barh(pop['original_title'].head(15), pop['popularity'].head(15), align='center', color='skyblue')
plt.gca().invert_yaxis()
plt.xlabel("Popularity")
plt.title("Popular Movies")

"""# **Content Based Filtering**

"""


#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'
tfidf = TfidfVectorizer(stop_words='english')

#Replace NaN with an empty string
df2['overview'] = df2['overview'].fillna('')

#Construct the required TF-IDF matrix by fitting and transforming the data
tfidf_matrix = tfidf.fit_transform(df2['overview'])

#Output the shape of tfidf_matrix
tfidf_matrix.shape

pd.DataFrame(tfidf_matrix.toarray()).head()


# Compute the cosine similarity matrix
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

#Construct a reverse map of indices and movie titles
indices = pd.Series(df2.index, index=df2['original_title']).drop_duplicates()

def get_recommendations(title, cosine_sim = cosine_sim, num = 10):
    '''
    Function that takes in movie title as input and outputs most similar movies
    '''
    
    # Get the index of the movie that matches the title
    idx = indices[title]

    # Get the pairwsie similarity scores of all movies with that movie
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the movies based on the similarity scores
    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse=True)

    # Get the scores of the 10 most similar movies
    sim_scores = sim_scores[1: num + 1]

    # Get the movie indices
    movie_indices = [i[0] for i in sim_scores]

    # Return the top 10 most similar movies
    return df2['original_title'].iloc[movie_indices]

get_recommendations('Interstellar')

get_recommendations('The Avengers')

"""## **Credits, Genres and Keywords Based Recommender**

"""

# take a look at the dataframe once again
df2.head()

# Parse the stringified features into their corresponding python objects


features = ['cast', 'crew', 'keywords', 'genres', 'production_companies']
for feature in features:
    df2[feature] = df2[feature].apply(literal_eval)

# Get the director's name from the crew feature. If director is not listed, return NaN
def get_director(x):
    director = [i['name'] for i in x if i['job'] == 'Director']
    return director[0] if director else np.nan

# Returns the list top 3 elements or entire list; whichever is more.
def get_list(x):
    if isinstance(x, list):
        return x[:3] if len(x) > 3 else x
    return []

# Define new director, cast, genres and keywords features that are in a suitable form.
df2['director'] = df2['crew'].apply(get_director)

features = ['cast', 'keywords', 'genres', 'production_companies']
for feature in features:
    df2[feature] = df2[feature].apply(get_list)

# Print the new features of the first 3 films
df2[['original_title', 'cast', 'director', 'keywords', 'genres', 'production_companies']].head(3)

# Function to convert all strings to lower case and strip names of spaces
def clean_data(x):
    cleaned_list = []
    if isinstance(x, list):
        for i in x:
            cleaned_list.append(str.lower(i.replace(" ", "")))
        return cleaned_list
    else:
        # Check if director exists. If not, return empty string
        if isinstance(x, str):
            return str.lower(x.replace(" ", ""))
        else:
            return ''

# Apply clean_data function to your features.
features = ['cast', 'keywords', 'director', 'genres', 'production_companies']

for feature in features:
    df2[feature] = df2[feature].apply(clean_data)

def create_concatenation(x):

    # create soup using f-string

    soup = f"{(' '.join(x['keywords']))} {(' '.join(x['cast']))} {x['director']} {x['director']} {(' '.join(x['genres']))} {(' '.join(x['production_companies']))}"
    return soup

df2['soup'] = df2.apply(create_concatenation, axis=1)

# Import CountVectorizer and create the count matrix


count = CountVectorizer(stop_words='english')
count_matrix = count.fit_transform(df2['soup'])

# Compute the Cosine Similarity matrix based on the count_matrix

cosine_sim2 = cosine_similarity(count_matrix, count_matrix)

# Reset index of our main DataFrame and construct reverse mapping as before
df2 = df2.reset_index()

get_recommendations('The Dark Knight Rises', cosine_sim2)

get_recommendations('The Godfather', cosine_sim2)

"""# **Collaborative Filtering**

### **Single Value Decomposition**
"""

#!pip install surprise


ratings = pd.read_csv('/content/drive/MyDrive/archive 2/ratings_small.csv')
movies_meta = pd.read_csv('/content/drive/MyDrive/archive 2/movies_metadata.csv')
ratings_test = ratings.sample(frac = 0.2, random_state=42)
reader = Reader()
ratings.head()

plt.figure(figsize=(5,10))
rate_num = ratings.groupby(["userId"])["movieId"].count()
rate_num.plot.box()
print("The median of the number of votes is", rate_num.median())


data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)
kf = KFold(n_splits=5)
kf.split(data)

svd = SVD(n_factors = 200, n_epochs = 30)
cross_validate(svd, data, measures = ['MSE', 'MAE'])

trainset = data.build_full_trainset()
svd.fit(trainset)

# a simple preprocessing of the dataset since we have weird "-" on movie id column
# we are expecting all the ids to be strings which can convert to int

movies_meta = movies_meta[~movies_meta["id"].str.contains("-")]

def get_user_recommendation(uid, num_movie = 10):
  '''
  Get the movie recommendation for a specific user 
  '''
  # all the movies
  id_range = [int(i) for i in movies_meta['id']]

  # create a series with index of id_range and value being the predicted SVD score
  film_id = pd.Series(data = [svd.predict(uid, i, 4)[3] for i in id_range], index = id_range) 

  # get the top movie id
  movie_indices = film_id.sort_values(ascending = False).index[:num_movie] 
  movie_indices = [str(i) for i in movie_indices]
  
  # refer to the movie_metadata, get the name of the movie
  return movies_meta[movies_meta['id'].isin(movie_indices)]['original_title']

def get_rated_movie(uid):
   '''
   return the name of the movies that a specific user rated, from score high to low
   '''
   id_lst = ratings[ratings['userId'] == uid].sort_values("rating", ascending = False)['movieId']
   id_lst = [str(i) for i in id_lst]
   return movies_meta[movies_meta['id'].isin(id_lst)]['original_title']


# try two examples

get_user_recommendation(1)

get_rated_movie(1)

get_user_recommendation(173)

get_rated_movie(173)

"""## **Conclusion** 

We create recommenders using demographic , content- based and collaborative filtering. We can see a promotion in performance as we try more advanced models (more work should be done to actually quantify the difference in performance, such as experiments or interviews). However, this recommender system is still very naive and have a potential to be improved given larger, more up-to-date dataset on both movies and users. There also exists numerous more advanced methods that can be used in making the recommendation.

"""

plt.figure(figsize=(12, 9))
weighted.groupby(['year'])['budget'].count().plot.bar(rot = 45)

"""We can see the peak of number of movies is on year 2012, only above 80. If I were to carry on an experiment to test the real accuracy of the system, I may have the movie in the data that my interviewees have seen, which put an obstacle on varifying the capacity of collaborative model.

Some alternative choices of dataset I would look into:


*   UpdatedTMDB Movies Dataset: https://www.kaggle.com/datasets/jillanisofttech/updatedtmdb-movies-dataset
*   Movie Dataset Rating (Latest Most Rated Movie Dataset): https://www.kaggle.com/datasets/ayushjain001/movie-dataset-rating

### Reference ###

*   This is the blog I learn about the SVD algorithm: https://towardsdatascience.com/intro-to-recommender-system-collaborative-filtering-64a238194a26
*   This is where I learn about TF-IDF: https://people.ischool.berkeley.edu/~dbamman/nlp20_slides/8_vector_semantics.pdf
"""